---
title: "R Workshop"
subtitle: "Module 5: Statistical modeling with R (1)"
author: "Bobae Kang (bobae.kang@illinois.gov)"
date: "April 18, 2018"
output:
  revealjs::revealjs_presentation:
    center: true
    transition: concave
    incremental: false
    css: ../css/style_revealjs.css
    reveal_options:
      slideNumber: true
    includes:
      in_header: header.html
---

```{r setup, include=FALSE}
library(knitr)
library(dplyr)
library(icjiar)
library(broom)
library(modelr)

opts_chunk$set(echo = TRUE)
```

## Agenda
<div style="text-align:center; margin-top:10%;">
<ul style="list-style: none">
<li style="color: #00061a; font-size: 1.1em; font-weight:700">
  Part 1: Basics of statistical modeling</li>
<li>
  Part 2: Options for advanced modeling</li>
</div>


## Basic Descriptive Statistics
```{r echo=FALSE, out.width="50%", out.extra='style="display: block; margin: auto; box-shadow: none;"'}
include_graphics("../images/vadlo_descriptive_statistics.gif")
```
<p style="font-size:0.5em; text-align:center; color: #777;">
Source: <a href="http://vadlo.com/cartoons.php?id=196">Vadlo.com</a>
</p>


## Quick summary stats
```{r eval=FALSE}
summary(data)
```
* Base R's `summary()` function is a quick way to get descriptive statistics on each columan of a tabular data object.
    * For numerical columns, we get minimum, 1st quartile, median, mean, 3rd quartie and maximum values, as well as the count of missing value (`NA`).
    * For categorical variables (e.g. `factor`), we can the count of each level as well as the missing value (`NA`).
* `summary()` is also used to get the "summary" of a fitted model object (e.g. `lm`) as well, as we will see shortly.


##
```{r}
summary(ispcrime)
```


## Numerical variables
* Central tendency
* Variability
* Shape
* Outliers


## Central tendency
* In statistics, central tendency is concerned with the "center" of a distribution. The following three are the common measures of central tendency:
  * "mean" for the arithmetic mean
  * "median" for the 50th percentile
  * "mode" for the most frequent value
* R has built-in functions for the first two: `mean()` and `median()`
  

##
```{r}
mean(ispcrime$violentCrime)
median(ispcrime$violentCrime)
```
<p style="text-align:center">... wait, what?</p>


##
```{r}
mean(ispcrime$violentCrime, na.rm = TRUE)
median(ispcrime$violentCrime, na.rm = TRUE)
```
* `NA` is known to be "contagious" in R, making any operation with missing values return `NA`
* `na.rm` is an argument with a boolean input to control whether `NA` values are removed/excluded in calculation


## Variability
* Variability (or dispersion) in statistics is concerned with the extent to which a distribution is spreaded out.
* There exist various measures for variability and R has built-in functions for many of them:
  * range: `min()`, `max()`, `range()`
  * percentiles: `fivenum()`, `IQR()`, `quantile()`
  * variance: `var()`, `sd()`


##
**Range**
```{r}
min(ispcrime$violentCrime, na.rm = TRUE)
max(ispcrime$violentCrime, na.rm = TRUE)
range(ispcrime$violentCrime, na.rm = TRUE)
```
* Range is the interval between the minium nad maximum values.


##
**Percentiles**
```{r}
fivenum(ispcrime$violentCrime, na.rm = TRUE)
IQR(ispcrime$violentCrime, na.rm = TRUE)
```
* `fivenum()` returns what is called Tukey's five number summary (minumum, 1st quartile, median, 3rd qurtile, maximum) for the input data
* `IQR()` returns the inter-quartile range, which is the difference between the 1st quartile (25%) and 3rd quartile (75%)


##
```{r}
# default: equal to fivenum()
quantile(ispcrime$violentCrime, probs = seq(0, 1, 0.25), na.rm = TRUE)
quantile(ispcrime$violentCrime, probs = seq(0, 1, 0.1), na.rm = TRUE)
```
* The default setting output of `quantile()` is equal to `fivenum()` output
* However, `quantile()` can use `probs` arugment to get more flexible output


##
**Variance**
```{r}
var(ispcrime$violentCrime, na.rm = TRUE)
sd(ispcrime$violentCrime, na.rm = TRUE)
```
* `var()` returns the sample variance of the given data:
    * $s^2 = (\sum{x - \bar{x}}^2)/(n-1)$
* `sd()` returns the standard deviation of the given data:
    * $s = \sqrt{(\sum{x - \bar{x}}^2)/(n-1)}$


##
```{r}
variables <- ispcrime %>% select(violentCrime, propertyCrime)
var(variables, na.rm = TRUE)
```
* When multiple variables are given as its input, `var()` returns a covariance matrix
    * In such cases, `cov()`can be used instead; however, `cov()` has no `na.rm` argument to remove missing values


## Shape
* The skewness and kurtosis (fatness/thinness) of a distribution are called the "shape" of the distribution.
* The `moments` package offers the following functions to meaasure the shape of a distribution:
    * `skewness()`
    * `kurtosis()`
    * See `moments` package [documentation](https://cran.r-project.org/web/packages/moments/moments.pdf)


## Outliers
* Outliers are observations or data points that are far from most other observations and disproportinately affect key summary statistics
* The `outliers` package offers useful functions to detect and measure outliers in data.
    * See `outliers` package [documentation](https://cran.r-project.org/web/packages/outliers/outliers.pdf)


## Categorical variables
* Frequency table is one of the most common ways to summarize the distribution of a catagorical variable
* Frequency tables can be made using table functions
    * `table()` for generating frequency tables
    * `prop.table()` for tables of proportions
    * `xtabs()` for creating frequency tables using formula
    * `ftable()` for creating "flat" contingency tables


## Table functions
```{r eval=FALSE}
table(...)
prop.table(x, margin = NULL)
ftable(x)
xtabs(formula, data, ...)
```
* `table()` takes one or more data vectors of same length
    * Each input data can be named
    * Use `as.data.frame()` to turn a `table` into a data frame 
* `prop.table()` and `ftable()` takes a `table` object
* `xtabs` use formula to generate a frequency table
    * If a data frame is provided as the `data` input, its column names can be used directly in formula


##
<br>
```{r}
my_data <- ispcrime %>%
  left_join(regions) %>%
  select(
    region,
    viol = violentCrime,
    prop = propertyCrime
  ) %>%
  mutate(
    high_viol = ifelse(viol > mean(viol, na.rm = TRUE), 1, 0),
    high_prop = ifelse(prop > mean(prop, na.rm = TRUE), 1, 0)
  )

my_tbl <- table(
  region = my_data$region,
  hviol = my_data$high_viol
)
```
***
<br>
```{r}
my_tbl
as.data.frame(my_tbl)
```


##
```{r}
prop.table(my_tbl, 1) # each row adds up to 1 
prop.table(my_tbl, 2) # each column adds up to 1
```


##
<br>
```{r}
my_tbl2 <- table(
  region = my_data$region,
  hviol = my_data$high_viol,
  hprop = my_data$high_prop
)

# with ftable
ftable(my_tbl2)
```
*** 
<br>
```{r}
# without ftable
my_tbl2
```


##
```{r}
# one-dimension
xtabs(~ region, my_data)

# two-dimension
xtabs(~ region + high_viol, my_data)
```


## Basic Inferential Statistics
```{r echo=FALSE, out.width="50%", out.extra='style="display: block; margin: auto; box-shadow: none;"'}
include_graphics("../images/vadlo_inferential_statistics.gif")
```
<p style="font-size:0.5em; text-align:center; color: #777;">
Source: <a href="http://vadlo.com/cartoons.php?id=407">Vadlo.com</a>
</p>


## Student's t-test
```{r eval=FALSE}
t.test(x, y = NULL, alternative = c("two.sided", "less", "greater"),
       mu = 0, conf.level = 0.95, ...)
t.test(formula, data, subset, na.action, ...)
```
* The null hypothesis assumes normal distribution
* The tested null hypothesis is no difference in mean and the alternative hypothesis can one of the three options, with the default of "two.sided"
* One sample t-test is conducted with only `x` input is given
    * The mean of `x` is compared against the `mu` value (default 0)
* Two sample t-test can be conducted by either supplying `x` and `y` inputs or using `formula` (`y ~ x`) with `data` input


##
```{r}
ispcrime2 <- left_join(ispcrime, regions)
viol_crime_north <- (filter(ispcrime2, region == "Northern"))$violentCrime
viol_crime_south <- (filter(ispcrime2, region == "Southern"))$violentCrime
t.test(viol_crime_north, viol_crime_south)
```


## Wilcoxon tests
```{r eval=FALSE}
wilcox.test(x, y = NULL, alternative = c("two.sided", "less", "greater"),
            mu = 0, conf.level = 0.95, ...)
wilcox.test(formula, data, subset, na.action, ...)
```
* The null hypothesis does NOT assume normality
* One sample Wilcoxon test (rank-sum test) is conducted with only `x` input is provided
* Two sample Wilcoxon test (singed-rank test) can be conducted by either supplying `x` and `y` inputs or using `formula` (`y ~ x`) with `data` input


## Analysis of Variance
```{r eval=FALSE}
aov(formula, data, qr = TRUE, ...)
```
* `formula` is of the following format: `value ~ subgroup`
* `summary()` has a method for `aov` class that returns a refined print result
* `model.tables()` has a method for `aov` class for reporting table of means or table of effects


##
```{r}
aov_by_region <- aov(formula = violentCrime ~ region, data = ispcrime2)
print(aov_by_region)
summary(aov_by_region)
```


##
```{r}
model.tables(aov_by_region, type = "means")
model.tables(aov_by_region, type = "effects")
```


## Other statistical tests
```{r eval=FALSE}
# Test of equal proprtions
prop.test(x, n, p, ...)

# Chi-square test of indepndence/goodness-of-fit
chisq.test(x, y = NULL, ...)

# Shapiro-Wilk test of normality
shapiro.test(x)

# One- or two-sample Kolmogorov-Smirnov test
ks.test(x, y, ..., alternative = "two.sided", exact = NULL)

# F-test to compare variances
var.test(x, y, ratio = 1, alternative = "two.sided",
         conf.level = 0.95, ...)
var.test(formula, data, subset, na.action, ...)

# Test of correlation between paired samples
cor.test(x, y, alternative = "two.sided", conf.level = 0.95,
         method = c("pearson", "kendell", "spearman"), ...)
```


## Linear models
<p style="font-size:1.5em">
$$y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$$
<br>
$$\boldsymbol{\text{y}} = \boldsymbol{\text{X}}^{\text{T}}\boldsymbol{\beta} + \boldsymbol{\varepsilon}$$
</p>


## The lm() function
```{r eval=FALSE}
lm(formula, data, subset, na.action, ...)
```
* `lm()` is used to fit linear models, according to the `formula` input.
* `data` is an optional argument, which can take a data frame
    * If `data` input is provided, its column names can be used directly in the formula


## R formula
```{r eval=FALSE}
formula <- y ~ x1 + x2
```
* The response variable is placed on the left-hand side of the tilde symbol (`~`), and the explanatory variables on the right-hand side
    * Adding multiple explanatory variables can be done using `+` symbol
* Simply use `y ~ .` to include all other columns additively as explanatory variables


## Model summary
```{r eval=FALSE}
lmfit <- lm(formula, data)

print(lmfit) # not recommended
summary(lmfit)
```
* `lm()` output is an object of `lm` class
* Simply printing an `lm` object gives only minimal information on the fitted model
* `summary()` function with a model input (e.g. `lm`) prints a more comprehensive model summary


##
```{r}
my_lmfit <- lm(violentCrime ~ propertyCrime, ispcrime)
class(my_lmfit)
my_lmfit
```


##
```{r}
summary(my_lmfit)
```


## Model diagnostics
```{r eval=FALSE}
plot(lmfit, which = c(1:3, 5), ...)
```
* Base R `plot()` function has a method for `lm` objects
* `Total 6 plots are generated
    * "Residuals vs Fitted"
    * "Normal Q-Q"
    * "Scale-Location"
    * "Cook's distance"
    * "Residuals vs Leverage"
    * "Cook's dist vs Leverage $h_{ii}/(1-h_ii)$"


##
```{r fig.width=11}
par(mfrow = c(2,3))
plot(my_lmfit, which = c(1:6))
```


## Model evaluation
```{r eval=FALSE}
logLik(object, ...)
AIC(object, ..., k = 2)
BIC(object, ...) # equivalent to AIC with k = log(n)
```
* There are measures of model fit other than $R^2$.
* For a model fitted by maximum likelihood algorithm:
    * `logLik()` for log-liklehood value
    * `AIC()` for Akaike Information Criterion (with default `k`)
    * `BIC()` for Bayesian Information Criterion
* Read [this Wikipedia article](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) for more on maximum likelihood estimation


## Extending lm() formula
* Transformations
* Interaction terms
* Polynomials
* Polynomial splines


## Transformations
```{r eval=FALSE}
scale(y) ~ log(x1) + sqrt(x2) + ...
```
* We can transform formula terms on either side of `~` by applying a function to each term
* Common transformations include:
    * `log()`: natural log (cf. `log2` and `log10`)
    * `exp()`: exponentiate
    * `sqrt()`: square root
    * `scale()`: rescale data such that the mean is 0 and the standard deviation is 1 


##
<br>
**Transforming `x`**
<br>
```{r echo=FALSE}
x <- seq(1, 100, 2)
y <- seq(1, 100, 2)
par(mfrow = c(2, 2))
plot(x, y)
abline(a = 0, b = 1, col = "red")
plot(log(x), y)
abline(a = 0, b = 1, col = "red")
plot(sqrt(x), y)
abline(a = 0, b = 1, col = "red")
plot(exp(x), y)
abline(a = 0, b = 1, col = "red")
```

***
<br>
**Transforming `y`**
<br>
```{r echo=FALSE}
par(mfrow = c(2, 2))
plot(x, y)
abline(a = 0, b = 1, col = "red")
plot(x, log(y))
abline(a = 0, b = 1, col = "red")
plot(x, sqrt(y))
abline(a = 0, b = 1, col = "red")
plot(x, exp(y))
abline(a = 0, b = 1, col = "red")
```


## Interactions
```{r eval=FALSE}
y ~ x1 + x2 + x1:x2
y ~ x1 * x2
```
* `x1:x2` takes the interactions of all terms in `x1` and `x2`
* `x1*x2` is equivalent to `x1:x2` *and* the original terms `x1` and `x2` 
* It is recommended to use an interaction term with all the original terms


## Polynomials
```{r eval=FALSE}
y ~ ploy(x, degree = n)
y ~ x + I(x^2) + I(x^3) + ...
```
* `poly()` generates the n-th degree polynimals of the input `x`
* `ploy(x, 3)` is equivalent to using `x + I(x^2) + I(x^3)`
    * `I()` ensures the `^` with a symbol is used as an arithmetic operator


## Polynomial splines
```{r}
library(splines) # part of R "base pacakges"
y ~ ns(x, ...)
y ~ bs(x, degree = n, ...)
```
* `ns()` is used for the natural cubic splines
* `bs()` is used for the n-th degree polynomial splines
    * equivalent to `bs(x, degree = 3)`


## broom package
* `broom` is a `tidyverse` package for "tidying up" statistical model outputs in R, i.e., converting them into tidy data frames.
* `broom` functions:
  * `glance()`
  * `tidy()`
  * `augment()`
* Visit `broom` [GitHub repository](https://github.com/tidyverse/broom) and "`broom` as well as `dplyr`" [vignette](https://cran.r-project.org/web/packages/broom/vignettes/broom_and_dplyr.html) for more


## broom functions
```{r eval=FALSE}
glance(x, ...)
tidy(x, ...)
augment(x, ...)
```
* `glance()` takes a model object and returns a concise one-row summary of the model.
* `tidy()` takes a model object and returns a `data.frame` with the coeffcient estimation results
* `augment()` takes a model object and returns a `data.frame` for each observation with additional columns such as predictions, residuals and cluster assignments


##
```{r}
glance(my_lmfit)
tidy(my_lmfit)
# check the class of each output
c(class(glance(my_lmfit)), class(glance(my_lmfit)))
```


##
```{r}
head(augment(my_lmfit))
class(augment(my_lmfit))
```


## modelr package
* `modelr` is a `tidyverse` package that provides helper functions for statistical modeling in R
  * partitioning and sampling
  * model quality metrics
  * interacting with models
* Visit `modelr` [GitHub repository](https://github.com/tidyverse/modelr) and the ["Model Basics" chapter](http://r4ds.had.co.nz/model-basics.html) in *R for Data Science* for more


## Partitioning and sampling
```{r eval=FALSE}
resample(data, idx)
resample_partition(data, p)
bootstrap(data, n, id = ".id")
```
* `resample` take a data frame and a vector of indices to create a pointer object, which can be either turned into a data frame or used directly in modeling functions (e.g. `lm()`)
* `resample_partition` takes a data and a named numeric vector to specify partitioning. Its output is a list of random partitions.
* `bootstrap` takes a data frame and an integer to specify the number of bootstrap replicates to generate. The output is a data frame of bootstrap samples column and id column


##
```{r}
set.seed(1) # for reproducibility

partitioned <- ispcrime %>%
  filter(year == "2015") %>%
  resample_partition(p = c(treatment = 0.5, control = 0.5))

partitioned
```


##
```{r}
head(partitioned$treatment$data)
partitioned$treatment$idx
```


## Model quality metrics
```{r eval=FALSE}
rmse(model, data)
mae(model, data)
qae(model, data, probs = c(0.05, 0.25, 0.5, 0.75, 0.95))
rsquare(model, data)
```
* `rmse()` returns the root mean squared error
* `mae()` returns the mean abosolute error
* `qae()` returns quantiles of absolute error
* `rsquare()` returns the $R^2$ value, i.e. the variance of the predictions divided by the variance of the reponse


## Interacting with models
```{r eval=FALSE}
add_predictions(data, model, var = "pred")
add_residuals(data, model, var = "resid")
```
* `add_predictions()` adds a new column to a data frame for predicted values based on a model
* `add_residuals()` adds a new column to a data frame for residuals based on a model


##
```{r}
ispcrime_subset <- select(ispcrime, year:violentCrime, propertyCrime)

ispcrime_subset %>%
  add_predictions(my_lmfit) %>%
  add_residuals(my_lmfit) %>%
  head()
```


## Generalized linear models
<p style="font-size:1.5em">
$$\text{E}[\boldsymbol{\text{Y}}] = \mu = g^{-1}(\boldsymbol{\text{X}\beta})$$
<br>
$$\text{Var}[\boldsymbol{\text{Y}}] = \text{V}[\mu] = \text{V}[g^{-1}(\boldsymbol{\text{X}}\boldsymbol{\beta})]$$
</p>


## GLM basics
* Generalized linear models (GLMs) extend the linear model to fit response variables with error distribution other than the Gaussian (normal) distribution
* Three components of GLM:
    * A probability distribution from the exponential family
    * A linear predictor, $\eta = \boldsymbol{\text{X}\beta}$
    * A link function $g$, such that $\text{E}[\boldsymbol{\text{Y}}] = \mu = g^{-1}(\eta)$
* See [this Wikipedia article](https://en.wikipedia.org/wiki/Generalized_linear_model) for more on GLM and the model components


## The glm() function
```{r eval=FALSE}
glm(formula, family = gaussian, data, ...)
```
* `formula` is the same formula we have seen all along
* `family` input specifies the link function $g$
    * the default is `gaussian(link = "identity")` for the linear model
* `glm()` function returns a `glm` class object
    * Use `summary()` to inspect the model estimation results


##
**`family` objects**
```{r echo=FALSE}
kable(
  tribble(
    ~`family`, ~"Default link function", ~Use,
    "`gaussian`","`(link = \"identity\")`", "Linear model for continous outcome<br>(ordinary linear regression; OLS)",
    "`binomial`","`(link = \"logit\")`", "Logit model for binary outcome<br>(logistic regression)",
    "`poisson`","`(link = \"log\"`)", "Poisson model for count outcome",
    "`Gamma`","`(link = \"inverse\")`", "...",
    "`inverse.gaussian`","`(link = \"1/mu^2\")`", "...",
    "`quasi`", "`(link = \"identitiy\")`", "...",
    "`quasibinomial`", "`(link = \"logit\")`", "...",
    "`quasipoisson`", "`(link = \"log\")`", "..."
  )
)
```


## Logit model
```{r eval=FALSE}
glm(formula, family = binomial, data, ...)
```
* A logit model, i.e. logistic regression, can be fitted using `glm()` with `family = binomial`
* The response variable must range between 0 and 1 (an error will be thrown otherwise)


## Poisson model
```{r eval=FALSE}
glm(formula, family = poisson, data, ...)
```
* A possion model can be fitted using `glm()` with `family = poisson`
* The response variable must not include negative values (an error will be thrown otherwise)


## Better ways?
There are thrid-party packages to provide additional fuctions and/or different interfaces to existing statistical analysis functions. Although I have little exposure to them, the following two might be worth checking:

* `psych` package
    * See [this package vignette](https://cran.r-project.org/web/packages/psych/vignettes/intro.pdf) by the package author
* `jmv` package
    * See [this package documentation page](https://www.jamovi.org/jmv/)


## Resources
* Kabacoff, R. ["Statistics"](https://www.statmethods.net/stats/index.html). *Quick-R*.
* Lane D. et al. [*Online Statistics: A Multimedia Course of Study*](http://onlinestatbook.com/2/index.html).
* Prabhakaran, S. [r-statistics.co](http://r-statistics.co/).
* UCLA Institute for Digital Research and Education. ["R"](https://stats.idre.ucla.edu/r/).
* University of Cincinnati. ["Descriptive analytics"](http://uc-r.github.io/descriptive). *UC Business Analytic R Programming Guide*.
* University of Cincinnati. ["Predictive analytics"](http://uc-r.github.io/predictive). *UC Business Analytic R Programming Guide*.
* Wollschlaeger, D. [*RExRepos*](http://dwoll.de/rexrepos/index.html).
* Yau, C. ["Elementary Statistics with R"](http://www.r-tutor.com/elementary-statistics). *R Tutorial*.


## Questions?
```{r echo=FALSE, out.width="60%", out.extra='style="display: block; margin: auto; box-shadow: none;"'}
include_graphics("https://media0.giphy.com/media/8lPSqcjcNjymIOS4Pm/giphy.gif")
```
<p style="font-size:0.5em; text-align:center; color: #777;">
Source: <a href="https://giphy.com/gifs/kpop-bts-8lPSqcjcNjymIOS4Pm">Giphy</a>
</p>


## References
<ul style="font-size: 0.6em; list-style-type:none">
  <li>Lane D. et al. (n.d.). <a href="http://onlinestatbook.com/2/index.html"><span style="font-style:italic">Online Statistics: A Multimedia Course of Study</span></a>.</li>
  <li>Prabhakaran, S. (n.d.). <a href="http://r-statistics.co/Statistical-Tests-in-R.html">"Statistic Tests"</a>. <i>r-statistics.co</i>.</li>
  <li><a href="https://github.com/tidyverse/broom">tidyverse/broom Github repository</a></li>
  <li><a href="https://github.com/tidyverse/modelr">tidyverse/modelr Github repository</a></li>
  <li>University of Cincinnati. <i><a href="http://uc-r.github.io/">UC Business Analytics R Programming Guide</a></i></li>
  <li>Wikipedia contributors. (2018). <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">"Generalized linear model"</a>. <i>Wikipedia, The Free Encyclopedia</i>.</li>
</ul>


##
```{r echo=FALSE, out.width="45%", out.extra='style="display: block; margin: auto; box-shadow: none;"'}
include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Ic_pause_circle_outline_48px.svg/2000px-Ic_pause_circle_outline_48px.svg.png")
```
<p style="font-size:0.5em; text-align:center; color: #777;">
Source: <a href="https://www.wikimedia.org">Wikimedia.org</a>
</p>