---
title: "R Workshop: Module 5 (1)"
author: "Bobae Kang"
date: "April 18, 2018"
output:
  html_document:
    self_contained: false
    lib_dir: ./notes_files
    toc: TRUE
    toc_float	:	TRUE
    toc_depth: 2
    includes:
      after_body: ../include_footer.html
css: ../css/style.css
---
<!-- fontawesome CDN -->
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

<!-- logo -->
<div class="logo">
  <a href="http://www.icjia.state.il.us/" target="_blank"><img src="../images/icjia.png" alt="ICJIA logo"></img></a>
</div>


```{r setup, include=FALSE}
library(knitr)
library(dplyr)
library(icjiar)
library(broom)
library(modelr)
opts_chunk$set(echo = TRUE)
```

This page contains the notes for **the first part of R Workshop Module 5: Statistical modeling with R**, which is part of the R Workshop series prepared by ICJIA Research Analyst [Bobae Kang](http://www.icjia.state.il.us/biographies/bobae-kang){target="_blank"} to enable and encourage ICJIA researchers to take advantage of R, a statistical programming language that is one of the most powerful modern research tools.

### Links
Click [**here**](../index.html) to go to the workshop **home page**.

Click [**here**](../modules.html) to go to the workshop **Modules page**.

Click [**here**](../slides/module5_slides1.html){target="_blank"} to view the accompanying **slides for Module 5, Part 1**.

Navigate to the other workshop materials:

<button class="btn" data-toggle="collapse" data-target="#collapse-navigate" aria-expanded="false" aria-controls="collapse-navigate">**SEE MORE**</button>

<div class="collapse mt-1" id="collapse-navigate">
<div style="padding:20px;">

* **Module 1: Introduction to R** ([**slides**](../slides/module1_slides.html),
[**note**](module1_notes.html))
* **Module 2: R basics**
    * Part 1 ([**slides**](../slides/module2_slides1.html), [**note**](module2_notes1.html))
    * Part 2 ([**slides**](../slides/module2_slides2.html), [**note**](module2_notes2.html))
* **Module 3: Data analysis with R**
    * Part 1 ([**slides**](../slides/module3_slides1.html), [**note**](module3_notes1.html))
    * Part 2 ([**slides**](../slides/module3_slides2.html), [**note**](module3_notes2.html))
* **Module 4: Data visualization with R**
    * Part 1 ([**slides**](../slides/module4_slides1.html), [**note**](module4_notes1.html))
    * Part 2 ([**slides**](../slides/module4_slides2.html), [**note**](module4_notes2.html))
* **Module 5: Statistical modeling with R**
    * Part 2 ([**slides**](../slides/module5_slides2.html), [**note**](module5_notes2.html))
* **Module 6: "To Infinity and Beyond"**
    * Part 1 ([**slides**](../slides/module6_slides1.html), [**note**](module6_notes1.html))
    * Part 2 ([**slides**](../slides/module6_slides2.html), [**note**](module6_notes2.html))

</div>
</div>


***


# Statistical modleing with R (1): Basics of statistical modeling

R is first designed as a programming language and an environment for statistics and data analysis. Accordingly, R provides as part of its base packages a host of handy tools for conducting statistical analysis. This part introduces the basics of descriptive and inferential statistics in R and touches on fitting linear and generalized linear models.


***


# Basic Descriptive Statistics
```{r echo=FALSE, out.width="50%", out.extra='style="display: block; margin: auto; box-shadow: none;"'}
include_graphics("../images/vadlo_descriptive_statistics.gif")
```
<p style="font-size:0.5em; text-align:center; color: #777;">
Source: <a href="http://vadlo.com/cartoons.php?id=196" target="_blank">Vadlo.com</a>
</p>


## Quick summary stats
```{r eval=FALSE}
summary(data)
```
The Base R `summary()` function is a quick way to get descriptive statistics on each columan of a tabular data object.

For numerical columns, we get the minimum, 1st quartile, median, mean, 3rd quartie and maximum values, as well as the count of missing value (`NA`).

For categorical variables (e.g. `factor`), we can the count for each level as well as the missing value (`NA`).

`summary()` is also used to get the "summary" of a fitted model object (e.g. `lm`) as well, as we will see shortly.

### Example
Let's take a look at `summary` of the `ispcrime` dataset.
```{r}
summary(ispcrime)
```


## Numerical variables
A distribution of numerical values can be described using statistics on the following characteristics:

* Central tendency
* Variability
* Shape
* Outliers

In the following, we will briefly discuss what each of these characteristics means and how we can get relevant statistics in R. 

### Central tendency
In statistics, central tendency is concerned with the "center" of a distribution. The following three are the common measures of central tendency:

* "mean" for the arithmetic mean
* "median" for the 50th percentile
* "mode" for the most frequent value

R has built-in functions for the first two: `mean()` and `median()`. Unfortunately, there is no native R function for mode. However, it is possible to create a custom function using the definition, i.e. the most frequnt value. Also, check out the `modeest` package that offers mode estimators.

Let us try using `mean()` and `median()` for the `violentCrime` column. And the result is ...
```{r}
mean(ispcrime$violentCrime)
median(ispcrime$violentCrime)
```
... wait, what? We get `NA` for both values.

This is because `NA` in R is a "contagious" value, making any operation with missing values return `NA`. To remedy this, we can use the `na.rm` argument, which takes a boolean input to control weather whether `NA` values are removed/excluded in calculation. The default is `na.rm = FALSE`

We try again with `na.rm = TRUE`:
```{r}
mean(ispcrime$violentCrime, na.rm = TRUE)
median(ispcrime$violentCrime, na.rm = TRUE)
```

### Variability
Variability (or dispersion) in statistics is concerned with the extent to which a distribution is spreaded out.

There exist various measures for variability and R has built-in functions for many of them:

* range: `min()`, `max()`, `range()`
* percentiles: `fivenum()`, `IQR()`, `quantile()`
* variance: `var()`, `sd()`

#### Range
Range is the interval between the minium nad maximum values. R offers `min` for getting the minimum value, `max()` for the maximum value, and `range()` for the range. Each of functions has the `na.rm` argument, so make sure change its input as needed.

```{r}
min(ispcrime$violentCrime, na.rm = TRUE)
max(ispcrime$violentCrime, na.rm = TRUE)
range(ispcrime$violentCrime, na.rm = TRUE)
```


#### Percentiles
R has a few useful functions to inspect the distribution of numerical data in terms of percentiles.

The `fivenum()` function returns what is called Tukey's five number summary (minumum, 1st quartile, median, 3rd qurtile, maximum) for the input data. `IQR()` returns the inter-quartile range, which is the difference between the 1st quartile (25%) and 3rd quartile (75%).

The following shows the outputs of these functions using `violentCrime` as the input:
```{r}
fivenum(ispcrime$violentCrime, na.rm = TRUE)
IQR(ispcrime$violentCrime, na.rm = TRUE)
```

`quntile()` is a somewhat more flexible function. The default setting output of `quantile()` is equal to `fivenum()` output. However, `quantile()` can use `probs` arugment to get more flexible output.
```{r}
# default: equal to fivenum()
quantile(ispcrime$violentCrime, probs = seq(0, 1, 0.25), na.rm = TRUE)
quantile(ispcrime$violentCrime, probs = seq(0, 1, 0.1), na.rm = TRUE)
```

#### Variance

The R function for variance is `var()`, which returns the sample variance of the given data:
$$s^2 = (\sum{x - \bar{x}}^2)/(n-1)$$

```{r}
var(ispcrime$violentCrime, na.rm = TRUE)
```

Similarly, `sd()` returns the standard deviation of the given data:
$$s = \sqrt{(\sum{x - \bar{x}}^2)/(n-1)}$$
```{r}
sd(ispcrime$violentCrime, na.rm = TRUE)
```

Note that `var()` returns a covariance matrix when multiple variables are given as its input. In such cases, another function named `cov()` can be used instead for clarity; however, `cov()` has no `na.rm` argument to handle missing values.

```{r}
variables <- ispcrime %>% select(violentCrime, propertyCrime)
var(variables, na.rm = TRUE)
```

### Shape

The skewness and kurtosis (fatness/thinness) of a distribution are called the "shape" of the distribution.

While base R has no functions to get these statistics (which are rarely used for common descriptive statistics anyway), the `moments` package offers the following functions to meaasure the shape of a distribution:

* `skewness()`
* `kurtosis()`

See `moments` package's [reference manual](https://cran.r-project.org/web/packages/moments/moments.pdf){target="_blank"} to find out more.

### Outliers

Outliers are observations or data points that are far from most other observations and disproportinately affect key summary statistics

The `outliers` package offers useful functions to detect and measure outliers in data. See `outliers` package's  [reference manual](https://cran.r-project.org/web/packages/outliers/outliers.pdf){target="_blank"} to learn more.


## Categorical variables
Inspecting the distribution of a categorical variable requires a different set of tools. Frequency table is one of the most common ways to summarize the distribution of a catagorical variable.

In R, frequency tables can be made using table functions. We will take a look at the following:

* `table()` for generating frequency tables
* `prop.table()` for tables of proportions
* `xtabs()` for creating frequency tables using formula
* `ftable()` for creating "flat" contingency tables

### Table functions
```{r eval=FALSE}
table(...)
prop.table(x, margin = NULL)
ftable(x)
xtabs(formula, data, ...)
```
`table()` takes one or more data vectors of same length. Each input data can be named like in the case of creating a `list` or a `data.frame` object. We can use `as.data.frame()` to turn a `table` into a data frame.

Both `prop.table()` and `ftable()` takes a `table` object. `prop.table()` returns a table in terms of proportion, where $0 \leq p \leq 1$ for each value. `ftable()` returns a `flat` contingency table. We will shorty see what this means with an example below.

Finally, `xtabs` use formula to generate a frequency table. If a data frame is provided as the `data` input, its column names can be used directly in formula.

### `table()` example
First, let us create a data to use for creating tables:
```{r}
my_data <- ispcrime %>%
  left_join(regions) %>%
  select(
    region,
    viol = violentCrime,
    prop = propertyCrime
  ) %>%
  mutate(
    high_viol = ifelse(viol > mean(viol, na.rm = TRUE), 1, 0),
    high_prop = ifelse(prop > mean(prop, na.rm = TRUE), 1, 0)
  )
```

Now, we create a `table` object with two dimensions: `region` for regions in the state, and `hviol` for the greater-than-average violent crime count.  
```{r}
my_tbl <- table(
  region = my_data$region,
  hviol = my_data$high_viol
)
```

Now we print the table.
```{r}
my_tbl
```

We can turn it into a `data.frame`, which can be more easily manipulated.
```{r}
as.data.frame(my_tbl)
```

### `prop.table()` example
`prop.table()` has a `margin` argument to control how to generate margin for. `magin` takes an index, or vector of indices as an input. Take a look at the following examples to see how `margin` argument works:
```{r}
prop.table(my_tbl, 1) # each row adds up to 1 
prop.table(my_tbl, 2) # each column adds up to 1
```
If `margin` input is `r NULL`, each cell is `x/sum(x)`.

### `ftable()` example
To better illustrate what `ftable()` does, we first create a table with three dimensions. This is becuase using `ftable()` for a table of 2 dimensions or less makes little difference.
```{r}
my_tbl2 <- table(
  region = my_data$region,
  hviol = my_data$high_viol,
  hprop = my_data$high_prop
)
```

Now, we first see the `ftable()` output.
```{r}
# with ftable
ftable(my_tbl2)
```

Then compare it to the default printing otuput.
```{r}
# without ftable
my_tbl2
```

### `xtabs()` example
`xtabs()` creates an `xtabs` object, which is built on top of `table`. Creating one-dimensional table with `xtabs()` looks like the following:
```{r}
# one-dimension
xtabs(~ region, my_data)
```

Adding more dimensions can be done usign the `+` operator in the formula:
```{r}
# two-dimension
xtabs(~ region + high_viol, my_data)
```


***


# Basic Inferential Statistics
```{r echo=FALSE, out.width="50%", out.extra='style="display: block; margin: auto; box-shadow: none;"'}
include_graphics("../images/vadlo_inferential_statistics.gif")
```
<p style="font-size:0.5em; text-align:center; color: #777;">
Source: <a href="http://vadlo.com/cartoons.php?id=407" target="_blank">Vadlo.com</a>
</p>


## Student's t-test
```{r eval=FALSE}
t.test(x, y = NULL, alternative = c("two.sided", "less", "greater"),
       mu = 0, conf.level = 0.95, ...)
t.test(formula, data, subset, na.action, ...)
```
R offers the `t.test()` function to conduct Student's t-test. `x` and `y` are numeric vectors of data values. 

If only `x` is given, one sample t-test is conducted and the mean of `x` is compared agianst the `mu` value with the default of 0. If both `x` and `y` are provided, two sample t-test is conducted.

The test null hypothesis is no difference in the sample means, assuming that each sample comes from a normal distribution.

`alternative` defines the alternative hypothesis, which can take the value of "two.sided" (default), "less", or "greater".

Two sample t-test can also be conducted by using `formula` (`y ~ x`) with `data` input.

### Example
Let's take a look at an example of using `t.test()`. First we prepae `x` and `y` to use.
```{r}
ispcrime2 <- left_join(ispcrime, regions)
viol_crime_north <- (filter(ispcrime2, region == "Northern"))$violentCrime
viol_crime_south <- (filter(ispcrime2, region == "Southern"))$violentCrime
```

Now we conduct two sample t-test using `viol_crime_north` and `viol_crime_south` with the default "two.sided" alternative hypothesis:
```{r}
t.test(viol_crime_north, viol_crime_south)
```

## Wilcoxon tests
```{r eval=FALSE}
wilcox.test(x, y = NULL, alternative = c("two.sided", "less", "greater"),
            mu = 0, conf.level = 0.95, ...)
wilcox.test(formula, data, subset, na.action, ...)
```
R offers `wilcox.test()` for Wilcoxon tests. It is often considered a nonparametric counterpart of the t-test, and can be performed using either one sample or two samples. Unliked in Student's t-test, Wilcoxon test does not assume normality.

One sample Wilcoxon test (i.e. rank-sum test) is conducted with only `x` input is provided.

Two sample Wilcoxon test (singed-rank test) can be conducted by either supplying `x` and `y` inputs or using `formula` (`y ~ x`) with `data` input.

## Analysis of Variance
```{r eval=FALSE}
aov(formula, data, qr = TRUE, ...)
```
`aov()` is the built-in R function to perform analysis of variance (ANOVA) to analyze the differences among group means.

Here we use `formula` in the following format: `value ~ subgroup`. `data` is a data.frame with both `value` and `subgroup` as its columns.

`summary()` has a method for `aov` class that returns a refined print result. Also, `model.tables()` has a method for `aov` class for reporting table of means or table of effects.


### Example
This example is to illustrate different printing methods for the ANOVA model results. We first fit the model, using `violentCrime` as the value column and `region` as the subgroup column.
```{r}
aov_by_region <- aov(formula = violentCrime ~ region, data = ispcrime2)
```

Here is the default printing output:
```{r}
print(aov_by_region)
```

And here is using `summary()`, of which the ouput includes the F statistic and the relevant p-value:
```{r}
summary(aov_by_region)
```

Finally, using `model.tables` with different `type` input, we get the tables of means and effects:
```{r}
model.tables(aov_by_region, type = "means")
model.tables(aov_by_region, type = "effects")
```


## Other statistical tests
The following is a list of other statistical tests with built-in R fuctions:
```{r eval=FALSE}
# Test of equal proprtions
prop.test(x, n, p, ...)

# Chi-square test of indepndence/goodness-of-fit
chisq.test(x, y = NULL, ...)

# Shapiro-Wilk test of normality
shapiro.test(x)

# One- or two-sample Kolmogorov-Smirnov test
ks.test(x, y, ..., alternative = "two.sided", exact = NULL)

# F-test to compare variances
var.test(x, y, ratio = 1, alternative = "two.sided",
         conf.level = 0.95, ...)
var.test(formula, data, subset, na.action, ...)

# Test of correlation between paired samples
cor.test(x, y, alternative = "two.sided", conf.level = 0.95,
         method = c("pearson", "kendell", "spearman"), ...)
```


## An alternative framework
* `infer` package is an attempt to "to perform statistical inference using an expressive statistical grammar that coheres with the tidyverse design framework."
* Watch the package author's presentation [video](https://www.rstudio.com/resources/videos/infer-a-package-for-tidy-statistical-inference/){target="_blank"}
* Visit the package [website](http://infer.netlify.com/index.html){target="_blank"} to learn more about it

```{r echo=FALSE, out.width="750%", out.extra='style="display: block; margin: auto; box-shadow: none;"'}
include_graphics("../images/infer.png")
```
<p style="font-size:0.5em; text-align:center; color: #777;">
Source: <a href="http://infer.netlify.com/index.html" target="_blank">infer package website</a>
</p>





***


# Linear models
<p style="font-size:1.5em">
$$y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$$
$$\boldsymbol{\text{y}} = \boldsymbol{\text{X}}^{\text{T}}\boldsymbol{\beta} + \boldsymbol{\varepsilon}$$
</p>

## The lm() function
```{r eval=FALSE}
lm(formula, data, subset, na.action, ...)
```
R offers `lm()` to fit linear models, one of the most fundamental yet powerful tools for statistical modeling. A linear model with a single explanatory variable is called a simple linear model. A linaer model with multiple explanatory variables is called muliple linear regression model.

A model to be fitted is specificed according to the `formula` input. `data` is an optional argument, which can take a data frame or one of its variants. If `data` input is provided, its column names can be used directly in the formula.


## R formula
```{r eval=FALSE}
formula <- y ~ x1 + x2
```

Here we take a closer look at the R `formula` object.

The response variable is placed on the left-hand side of the tilde symbol (`~`), and the explanatory variables on the right-hand side.

Adding multiple explanatory variables can be done using `+` symbol. We can simply use `y ~ .` to include all other columns additively as expanatory variables.


## Model summary
```{r eval=FALSE}
lmfit <- lm(formula, data)

print(lmfit) # not recommended
summary(lmfit)
```
An output of `lm()` is an R object of `lm` class. Simply printing an `lm` object gives only minimal information on the fitted model.

In order to get a more compresensive model summary, we can we `summary()` function with a model input (e.g. `lm`).

### Example
We fit a simple linear model with `violentCrime` as the reponse variable and `propertyCrime` as the explanatory variable. The class of the fitted model object is `lm`.
```{r}
my_lmfit <- lm(violentCrime ~ propertyCrime, ispcrime)
class(my_lmfit)
```

Here is the output for using the default printing method:
```{r}
my_lmfit
```

And here is using `summary()`:
```{r}
summary(my_lmfit)
```


## Model diagnostics
```{r eval=FALSE}
plot(lmfit, which = c(1:3, 5), ...)
```
Base R `plot()` function has a method for `lm` objects, which generates the following 6 diagnostics plots:

* "Residuals vs Fitted"
* "Normal Q-Q"
* "Scale-Location"
* "Cook's distance"
* "Residuals vs Leverage"
* "Cook's dist vs Leverage $h_{ii}/(1-h_ii)$"

The default option only prints the first, second, third and fifth of these plots.

### Example
Let's take a look at all six diagnostic plots for our fitted model:
```{r fig.height=7, fig.width=8}
par(mfrow = c(2,3))
plot(my_lmfit, which = c(1:6))
```


## Model evaluation
```{r eval=FALSE}
logLik(object, ...)
AIC(object, ..., k = 2)
BIC(object, ...) # equivalent to AIC with k = log(n)
```
There are measures of model fit other than $R^2$. For a model fitted by maximum likelihood algorithm, including the `lm` model, the following functions are available to get goodness-of-fit measures:

* `logLik()` for log-liklehood value
* `AIC()` for Akaike Information Criterion (with default `k`)
* `BIC()` for Bayesian Information Criterion

We can read [this Wikipedia article](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation){target="_blank"} for more on maximum likelihood estimation.


## Extending lm() formula
A linear model can be extended with simple "tricks" to better fit the data. Four such tricks include:

* Transformating model terms
* Including interaction terms
* Polynomials
* Polynomial splines

### Transformations
```{r eval=FALSE}
scale(y) ~ log(x1) + sqrt(x2) + ...
```
We can transform terms on either side of `~` in the formula to get a better model. Common such transformations include:

* `log()`: natural log (cf. `log2` and `log10`)
* `exp()`: exponentiate
* `sqrt()`: square root
* `scale()`: rescale data such that the mean is 0 and the standard deviation is 1

The following two figures offer a visual illustration of the effect of applying some of these transformations.

#### Transforming `x`
```{r echo=FALSE, fig.height=8}
x <- seq(1, 100, 2)
y <- seq(1, 100, 2)
par(mfrow = c(2, 2))
plot(x, y)
abline(a = 0, b = 1, col = "red")
plot(log(x), y)
abline(a = 0, b = 1, col = "red")
plot(sqrt(x), y)
abline(a = 0, b = 1, col = "red")
plot(exp(x), y)
abline(a = 0, b = 1, col = "red")
```

#### Transforming `y`
```{r echo=FALSE, fig.height=8}
par(mfrow = c(2, 2))
plot(x, y)
abline(a = 0, b = 1, col = "red")
plot(x, log(y))
abline(a = 0, b = 1, col = "red")
plot(x, sqrt(y))
abline(a = 0, b = 1, col = "red")
plot(x, exp(y))
abline(a = 0, b = 1, col = "red")
```

### Interactions
```{r eval=FALSE}
y ~ x1 + x2 + x1:x2
y ~ x1 * x2
```
Interaction terms are used to account for any significant interaction between explanatory variables where the effect of one explanatory variable on the response variable depends on the value of another explanatory variable.

In R formula, `x1:x2` takes the interactions of all terms in `x1` and `x2`. Additionally, `x1*x2` is equivalent to `x1:x2` *and* the original terms `x1` and `x2`. 

Please note that it is generally recommended to keep all the original terms if their interaction term is included.

### Polynomials
```{r eval=FALSE}
y ~ ploy(x, degree = n)
y ~ x + I(x^2) + I(x^3) + ...
```

* `poly()` generates the n-th degree polynimals of the input `x`
* `ploy(x, 3)` is equivalent to using `x + I(x^2) + I(x^3)`
    * `I()` ensures the `^` with a symbol is used as an arithmetic operator

### Polynomial splines
```{r eval=FALSE}
library(splines) # part of R "base pacakges"
y ~ ns(x, ...)
y ~ bs(x, degree = n, ...)
```
* `ns()` is used for the natural cubic splines
* `bs()` is used for the n-th degree polynomial splines
    * equivalent to `bs(x, degree = 3)`


## `broom` package
`broom` is a `tidyverse` package for "tidying up" statistical model outputs in R, i.e., converting them into tidy data frames.

Visit `broom` [GitHub repository](https://github.com/tidyverse/broom){target="_blank"} and "`broom` as well as `dplyr`" [vignette](https://cran.r-project.org/web/packages/broom/vignettes/broom_and_dplyr.html){target="_blank"} to learn more about the package.


### `broom` functions
```{r eval=FALSE}
glance(x, ...)
tidy(x, ...)
augment(x, ...)
```
`broom` is a small package consists of only the following three functions: 

* `glance()` takes a model object and returns a concise one-row summary of the model.
* `tidy()` takes a model object and returns a `data.frame` with the coeffcient estimation results
* `augment()` takes a model object and returns a `data.frame` for each observation with additional columns such as predictions, residuals and cluster assignments

### Example
Let's try these functions to see how they work. We will use `my_lmfit` and apply each `broom` function to the model.

First, we use `glance()` to see the model summary metrics in a tidy format.
```{r}
glance(my_lmfit)
```

Next, `tidy` returns the model estimates in a tidy format.
```{r}
tidy(my_lmfit)
```

And the outputs really are of `data.frame` class
```{r}
# check the class of each output
c(class(glance(my_lmfit)), class(glance(my_lmfit)))
```

Finally, we use `augment` to add to the input data additional columns based on the fitted model:
```{r}
head(augment(my_lmfit))
```

The output is, again, a tidy `data.frame` object.
```{r}
class(augment(my_lmfit))
```


## `modelr` package
`modelr` is a `tidyverse` package that provides helper functions for statistical modeling in R. Its helper functions are designed to facilitate the following:

* partitioning and sampling
* model quality metrics
* interacting with models

You can visit `modelr` [GitHub repository](https://github.com/tidyverse/modelr){target="_blank"} or read the ["Model Basics" chapter](http://r4ds.had.co.nz/model-basics.html){target="_blank"} in *R for Data Science* to learn more about the package.

### Partitioning and sampling
```{r eval=FALSE}
resample(data, idx)
resample_partition(data, p)
bootstrap(data, n, id = ".id")
```
* `resample` take a data frame and a vector of indices to create a pointer object, which can be either turned into a data frame or used directly in modeling functions (e.g. `lm()`)

* `resample_partition` takes a data and a named numeric vector to specify partitioning. Its output is a list of random partitions.
* `bootstrap` takes a data frame and an integer to specify the number of bootstrap replicates to generate. The output is a data frame of bootstrap samples column and id column

#### `resample_partition()` example
`resample_partition()` is suited for creating randomly assigned treatment/control groups. The output is a nested list of partitions. Note that resampling is a "random" process and the output will change each time the code is executed. To ensure reproducibility (i.e. the same result), use `set.seed()`.
```{r}
set.seed(1) # for reproducibility

partitioned <- ispcrime %>%
  filter(year == "2015") %>%
  resample_partition(p = c(treatment = 0.5, control = 0.5))

partitioned
```

Each partition is also a list, which contains `data` and `idx`. The former is a `data.frame` of the partition, the the latter is a numeric vector of indices assigned to the partition.
```{r}
head(partitioned$treatment$data)
partitioned$treatment$idx
```


### Model quality metrics
```{r eval=FALSE}
rmse(model, data)
mae(model, data)
qae(model, data, probs = c(0.05, 0.25, 0.5, 0.75, 0.95))
rsquare(model, data)
```
`modelr` offers the following functions to quickly extract model quality metrics: 

* `rmse()` returns the root mean squared error
* `mae()` returns the mean abosolute error
* `qae()` returns quantiles of absolute error
* `rsquare()` returns the $R^2$ value, i.e. the variance of the predictions divided by the variance of the reponse

### Interacting with models
```{r eval=FALSE}
add_predictions(data, model, var = "pred")
add_residuals(data, model, var = "resid")
```
* `add_predictions()` adds a new column to a data frame for predicted values based on a model
* `add_residuals()` adds a new column to a data frame for residuals based on a model

#### Example
Let's see how `add_predictions()` and `add_residuals()` work with an example. To make it easier to view, we first get a subset of `ispcrime`
```{r}
ispcrime_subset <- select(ispcrime, year:violentCrime, propertyCrime)
```

Now we use both functiosn to add prediction and residual columns. Note that the residual values equal true values - predicted values.
```{r}
ispcrime_subset %>%
  add_predictions(my_lmfit) %>%
  add_residuals(my_lmfit) %>%
  head()
```


***


# Generalized linear models
<p style="font-size:1.5em">
$$\text{E}[\boldsymbol{\text{Y}}] = \mu = g^{-1}(\boldsymbol{\text{X}\beta})$$
$$\text{Var}[\boldsymbol{\text{Y}}] = \text{V}[\mu] = \text{V}[g^{-1}(\boldsymbol{\text{X}}\boldsymbol{\beta})]$$
</p>


## GLM basics
The generalized linear model (GLM) extends the linear model to fit response variables with error distribution other than a gaussian distribution.

Three components of GLM are:

* A probability distribution from the exponential family
* A linear predictor, $\eta = \boldsymbol{\text{X}\beta}$
* A link function $g$, such that $\text{E}[\boldsymbol{\text{Y}}] = \mu = g^{-1}(\eta)$

See [this Wikipedia article](https://en.wikipedia.org/wiki/Generalized_linear_model){target="_blank"} for more on GLM and the model components.


## The glm() function
```{r eval=FALSE}
glm(formula, family = gaussian, data, ...)
```
R offers the `glm()` function to fit GLM models. `glm()` uses `formula` to specify the model. Then, it uses `family` input to specify the link function $g$. The default is `gaussian(link = "identity")`, which corresponds to the linear model.

`glm()` function returns a `glm` class object, which has a `summary()` method to inspect the model estimation results.

### `family` objects
The following table lists GLM `family` options available for `glm()`:
```{r echo=FALSE}
kable(
  tribble(
    ~`family`, ~"Default link function", ~Use,
    "`gaussian`","`(link = \"identity\")`", "Linear model for continous outcome<br>(ordinary linear regression; OLS)",
    "`binomial`","`(link = \"logit\")`", "Logit model for binary outcome<br>(logistic regression)",
    "`poisson`","`(link = \"log\"`)", "Poisson model for count outcome",
    "`Gamma`","`(link = \"inverse\")`", "...",
    "`inverse.gaussian`","`(link = \"1/mu^2\")`", "...",
    "`quasi`", "`(link = \"identitiy\")`", "...",
    "`quasibinomial`", "`(link = \"logit\")`", "...",
    "`quasipoisson`", "`(link = \"log\")`", "..."
  )
)
```

### Logit model
```{r eval=FALSE}
glm(formula, family = binomial, data, ...)
```
A logit model, i.e. logistic regression, can be fitted using `glm()` with `family = binomial`. The response variable must range between 0 and 1 (an error will be thrown otherwise).

### Poisson model
```{r eval=FALSE}
glm(formula, family = poisson, data, ...)
```
A possion model can be fitted using `glm()` with `family = poisson`. Here, the response variable must not include negative values (an error will be thrown otherwise).


## Better ways?
There are thrid-party packages to provide additional fuctions and/or different interfaces to existing statistical analysis functions. Although I have little exposure to them, the following two might be worth checking:

* `psych` package. See [this package vignette](https://cran.r-project.org/web/packages/psych/vignettes/intro.pdf){target="_blank"} by the package author to learn more about it.

* `jmv` package. See [this package documentation page](https://www.jamovi.org/jmv/){target="_blank"} to learn more about it.


## Resources

Just as in any previous work module parts, what we have seen in this part is only an introduction tot he topic. The following are some useful resources that would help you to deepen your understanding of the basic statistics in R.

* Kabacoff, R. ["Statistics"](https://www.statmethods.net/stats/index.html){target="_blank"}. *Quick-R*.
* Lane D. et al. [*Online Statistics: A Multimedia Course of Study*](http://onlinestatbook.com/2/index.html){target="_blank"}.
* Prabhakaran, S. [r-statistics.co](http://r-statistics.co/){target="_blank"}.
* UCLA Institute for Digital Research and Education. ["R"](https://stats.idre.ucla.edu/r/){target="_blank"}.
* University of Cincinnati. ["Descriptive analytics"](http://uc-r.github.io/descriptive){target="_blank"}. *UC Business Analytic R Programming Guide*.
* University of Cincinnati. ["Predictive analytics"](http://uc-r.github.io/predictive){target="_blank"}. *UC Business Analytic R Programming Guide*.
* Wollschlaeger, D. [*RExRepos*](http://dwoll.de/rexrepos/index.html){target="_blank"}.
* Yau, C. ["Elementary Statistics with R"](http://www.r-tutor.com/elementary-statistics){target="_blank"}. *R Tutorial*.




***


# References
<ul>
  <li>Lane D. et al. (n.d.). <a href="http://onlinestatbook.com/2/index.html" target="_blank"><i>Online Statistics: A Multimedia Course of Study</i></a>.</li>
  <li>Prabhakaran, S. (n.d.). <a href="http://r-statistics.co/Statistical-Tests-in-R.html" target="_blank">"Statistic Tests"</a>. <i>r-statistics.co</i>.</li>
  <li><a href="https://github.com/tidyverse/broom" target="_blank">tidyverse/broom Github repository</a></li>
  <li><a href="https://github.com/tidyverse/modelr" target="_blank">tidyverse/modelr Github repository</a></li>
  <li>University of Cincinnati. <i><a href="http://uc-r.github.io/" target="_blank">UC Business Analytics R Programming Guide</a></i></li>
  <li>Wikipedia contributors. (2018). <a href="https://en.wikipedia.org/wiki/Generalized_linear_model" target="_blank">"Generalized linear model"</a>. <i>Wikipedia, The Free Encyclopedia</i>.</li>
</ul>